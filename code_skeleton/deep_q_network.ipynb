{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"assignment2.ipynb","provenance":[{"file_id":"1_6i5zf8qZuoWEpxCTG2sosIXvjgxPWZr","timestamp":1639396234699}],"collapsed_sections":[],"authorship_tag":"ABX9TyP4KWS/96uRzmyEpmpeH6OB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8x6HnSY6SXIu"},"outputs":[],"source":["import sys, os\n","import gym\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import urllib.request\n","urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\n","!pip install unrar\n","!unrar x Roms.rar\n","!mkdir rars\n","!mv HC\\ ROMS.zip   rars\n","!mv ROMS.zip  rars\n","!python -m atari_py.import_roms rars"]},{"cell_type":"code","source":["import torch\n","import torchvision\n","import numpy as np\n","import random\n","import gym\n","from gym.spaces import Box\n","from collections import deque\n","\n","\n","class SkipFrame(gym.Wrapper):\n","    def __init__(self, env, skip):\n","        super().__init__(env)\n","        self._skip = skip\n","\n","    def step(self, action):\n","        total_reward = 0.0\n","        done = False\n","        for i in range(self._skip):\n","            obs, reward, done, info = self.env.step(action)\n","            total_reward += reward\n","            if done:\n","                break\n","        return obs, total_reward, done, info\n","\n","\n","class GrayScaleObservation(gym.ObservationWrapper):\n","    def __init__(self, env):\n","        super().__init__(env)\n","        obs_shape = self.observation_space.shape[:2]\n","        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n","\n","    def observation(self, observation):\n","        observation = np.transpose(observation, (2, 0, 1))\n","        observation = torch.tensor(observation.copy(), dtype=torch.float)\n","        transform = torchvision.transforms.Grayscale()\n","        observation = transform(observation)\n","        return observation\n","\n","\n","class ResizeObservation(gym.ObservationWrapper):\n","    def __init__(self, env, shape):\n","        super().__init__(env)\n","        self.shape = (shape, shape) if isinstance(shape, int) else tuple(shape)\n","        obs_shape = self.shape + self.observation_space.shape[2:]\n","        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n","\n","    def observation(self, observation):\n","        transforms = torchvision.transforms.Compose([torchvision.transforms.Resize(self.shape),\n","                                                     torchvision.transforms.Normalize(0, 255)])\n","        return transforms(observation).squeeze(0)\n","\n","\n","class ExperienceReplayMemory(object):\n","    def __init__(self, capacity):\n","        self.memory = deque([], maxlen=capacity)\n","\n","    def __len__(self):\n","        return len(self.memory)\n","\n","    def store(self, state, next_state, action, reward, done):\n","        state = state.__array__()\n","        next_state = next_state.__array__()\n","        self.memory.append((state, next_state, action, reward, done))\n","\n","    def sample(self, batch_size):\n","        # TODO: uniformly sample batches of Tensors for: state, next_state, action, reward, done\n","        # ...\n","        \n","        return torch.tensor(state), torch.tensor(next_state), torch.tensor(action), torch.tensor(reward), torch.tensor(done)"],"metadata":{"id":"GXInlvyWS8Tz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import gym\n","import numpy as np\n","import copy\n","from gym.wrappers import FrameStack\n","\n","\n","env = gym.make(\"BreakoutNoFrameskip-v4\")\n","env = SkipFrame(env, skip=4)\n","env = GrayScaleObservation(env)\n","env = ResizeObservation(env, shape=84)\n","env = FrameStack(env, num_stack=4)\n","image_stack, h, w = env.observation_space.shape\n","num_actions = env.action_space.n\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","seed = 61\n","env.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","if torch.backends.cudnn.enabled:\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","# Parameters\n","batch_size = 32\n","alpha = 0.00025\n","gamma = 0.99\n","eps, eps_decay = 1.0, 0.999\n","max_train_episodes = 1000000\n","max_test_episodes = 10\n","max_train_frames = 10000\n","burn_in_phase = 50000\n","sync_target = 10000\n","curr_step = 0\n","buffer = ExperienceReplayMemory(50000)\n"],"metadata":{"id":"5rWwNN4RTCix"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def convert(x):\n","    return torch.tensor(x.__array__()).float()\n","\n","\n","class DeepQNet(torch.nn.Module):\n","    def __init__(self, h, w, image_stack, num_actions):\n","        super(DeepQNet, self).__init__()\n","        # TODO: create a convolutional neural network\n","        # ...\n","\n","    def forward(self, x):\n","        # TODO: forward pass from the neural network\n","        # ...\n","\n","\n","# TODO: create an online and target DQN (Hint: Use copy.deepcopy() and requires_grad utilities!)\n","# ...\n","online_dqn = ...\n","target_dqn = ...\n","online_dqn.to(device)\n","target_dqn.to(device)\n","\n","\n","# TODO: create the appropriate MSE criterion and Adam optimizer\n","# ...\n","optimizer = ...\n","criterion = ...\n"],"metadata":{"id":"zt0LBAX1TYEZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def policy(state, is_training):\n","    global eps\n","    state = convert(state).unsqueeze(0).to(device)\n","\n","    # TODO: Implement an epsilon-greedy policy\n","    # ...\n","    pass\n","\n","\n","def compute_loss(state, action, reward, next_state, done):\n","    state = convert(state).to(device)\n","    next_state = convert(next_state).to(device)\n","    action = action.to(device)\n","    reward = reward.to(device)\n","    done = done.to(device)\n","    \n","    # TODO: Compute the DQN (or DDQN) loss based on the criterion\n","    # ...\n","    pass\n","\n","\n","def run_episode(curr_step, buffer, is_training, is_rendering=False):\n","    global eps\n","    episode_reward, episode_loss = 0, 0.\n","    state = env.reset()\n","    if is_rendering:\n","        env.render(\"rgb_array\")\n","\n","    for t in range(max_train_frames):\n","        action = policy(state, is_training)\n","        curr_step += 1\n","\n","        next_state, reward, done, _ = env.step(action)\n","        if is_rendering:\n","            env.render(\"rgb_array\")\n","\n","        episode_reward += reward\n","\n","        if is_training:\n","            buffer.store(state, next_state, action, reward, done)\n","\n","            if curr_step > burn_in_phase:\n","                state_batch, next_state_batch, action_batch, reward_batch, done_batch = buffer.sample(batch_size)\n","\n","                if curr_step % sync_target == 0:\n","                    # TODO: Periodically update your target_dqn at each sync_target frames\n","                    # ...\n","                    pass\n","\n","                loss = compute_loss(state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","                episode_loss += loss.item()\n","        else:\n","            with torch.no_grad():\n","                episode_loss += compute_loss(state, action, reward, next_state, done).item()\n","\n","        state = next_state\n","\n","        if done:\n","            break\n","\n","    return dict(reward=episode_reward, loss=episode_loss / t)\n","\n","\n","def update_metrics(metrics, episode):\n","    for k, v in episode.items():\n","        metrics[k].append(v)\n","\n","\n","def print_metrics(it, metrics, is_training, window=100):\n","    reward_mean = np.mean(metrics['reward'][-window:])\n","    loss_mean = np.mean(metrics['loss'][-window:])\n","    mode = \"train\" if is_training else \"test\"\n","    print(f\"Episode {it:4d} | {mode:5s} | reward {reward_mean:5.5f} | loss {loss_mean:5.5f}\")\n"],"metadata":{"id":"djecxEH6TbjC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_metrics = dict(reward=[], loss=[])\n","for it in range(max_train_episodes):\n","    episode_metrics = run_episode(curr_step, buffer, is_training=True)\n","    update_metrics(train_metrics, episode_metrics)\n","    if it % 10 == 0:\n","        print_metrics(it, train_metrics, is_training=True)\n","    eps *= eps_decay"],"metadata":{"id":"jnEqGWZcTcsy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_metrics = dict(reward=[], loss=[])\n","for it in range(max_test_episodes):\n","    episode_metrics = run_episode(buffer, is_training=False)\n","    update_metrics(test_metrics, episode_metrics)\n","    print_metrics(it + 1, test_metrics, is_training=False)\n","\n","# TODO: Plot your train_metrics and test_metrics\n","# ...\n"],"metadata":{"id":"0q9NICzAVvb_"},"execution_count":null,"outputs":[]}]}